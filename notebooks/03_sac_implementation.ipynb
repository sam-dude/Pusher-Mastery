{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - SAC Implementation from Scratch\n",
    "\n",
    "**Goal:** Implement Soft Actor-Critic (SAC) algorithm step-by-step to deeply understand how it works.\n",
    "\n",
    "**Time:** 3-4 hours\n",
    "\n",
    "**What you'll learn:**\n",
    "- The SAC algorithm architecture\n",
    "- How actor and critic networks work together\n",
    "- The role of entropy in exploration\n",
    "- Replay buffer mechanics\n",
    "- Target network updates\n",
    "\n",
    "**Why SAC?**\n",
    "- State-of-the-art for continuous control\n",
    "- Sample efficient\n",
    "- Stable training through entropy regularization\n",
    "- Works well on robotic tasks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T21:48:38.985729Z",
     "start_time": "2025-10-11T21:48:33.456941Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    from tqdm import tqdm\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import sys\n",
    "import os\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "# Plotting setup\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "print(\"‚úÖ Libraries imported\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è  Using device: cpu\n",
      "‚úÖ Libraries imported\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SAC Theory Overview\n",
    "\n",
    "### What is Soft Actor-Critic?\n",
    "\n",
    "SAC is an **off-policy** actor-critic algorithm that maximizes both:\n",
    "1. **Expected return** (cumulative reward)\n",
    "2. **Entropy** (exploration/randomness)\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **Actor (Policy) Network** üé≠\n",
    "   - Outputs mean and log_std for Gaussian policy\n",
    "   - Samples actions with reparameterization trick\n",
    "   - Goal: Maximize Q-value + entropy\n",
    "\n",
    "2. **Critic (Q) Networks** üéØ\n",
    "   - Two Q-networks (to reduce overestimation bias)\n",
    "   - Estimates value of state-action pairs\n",
    "   - Goal: Minimize TD error\n",
    "\n",
    "3. **Target Networks** üéØüéØ\n",
    "   - Slow-moving copies of Q-networks\n",
    "   - Stabilizes training\n",
    "   - Updated with polyak averaging\n",
    "\n",
    "4. **Entropy Temperature (Œ±)** üå°Ô∏è\n",
    "   - Controls exploration vs exploitation\n",
    "   - Can be learned automatically\n",
    "   - Higher Œ± ‚Üí more exploration\n",
    "\n",
    "5. **Replay Buffer** üíæ\n",
    "   - Stores past experiences\n",
    "   - Enables off-policy learning\n",
    "   - Breaks temporal correlations\n",
    "\n",
    "### The SAC Objective:\n",
    "\n",
    "```\n",
    "J(œÄ) = E[Œ£ r(s,a) + Œ±¬∑H(œÄ(¬∑|s))]\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `r(s,a)` = reward\n",
    "- `Œ±` = temperature parameter\n",
    "- `H(œÄ)` = entropy of policy\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Replay Buffer Implementation\n",
    "\n",
    "The replay buffer stores transitions `(s, a, r, s', done)` and samples random batches for training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T21:48:41.148514Z",
     "start_time": "2025-10-11T21:48:41.124220Z"
    }
   },
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Simple replay buffer for storing and sampling transitions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity=1000000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            capacity: Maximum number of transitions to store\n",
    "        \"\"\"\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Add a transition to the buffer.\n",
    "\n",
    "        Args:\n",
    "            state: Current state\n",
    "            action: Action taken\n",
    "            reward: Reward received\n",
    "            next_state: Next state\n",
    "            done: Whether episode terminated\n",
    "        \"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Sample a random batch of transitions.\n",
    "\n",
    "        Args:\n",
    "            batch_size: Number of transitions to sample\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (states, actions, rewards, next_states, dones)\n",
    "        \"\"\"\n",
    "        # Random sampling\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        # Unzip the batch\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        states = np.array(states)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards).reshape(-1, 1)\n",
    "        next_states = np.array(next_states)\n",
    "        dones = np.array(dones).reshape(-1, 1)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return current size of buffer.\"\"\"\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def is_ready(self, batch_size):\n",
    "        \"\"\"Check if buffer has enough samples for training.\"\"\"\n",
    "        return len(self.buffer) >= batch_size\n",
    "\n",
    "print(\"‚úÖ Replay Buffer implemented\")\n",
    "\n",
    "# Test the replay buffer\n",
    "print(\"\\nüß™ Testing Replay Buffer...\")\n",
    "test_buffer = ReplayBuffer(capacity=100)\n",
    "\n",
    "# Add some dummy transitions\n",
    "for i in range(10):\n",
    "    state = np.random.randn(23)\n",
    "    action = np.random.randn(7)\n",
    "    reward = np.random.randn()\n",
    "    next_state = np.random.randn(23)\n",
    "    done = False\n",
    "    test_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "print(f\"Buffer size: {len(test_buffer)}\")\n",
    "print(f\"Can sample batch of 5: {test_buffer.is_ready(5)}\")\n",
    "\n",
    "# Sample a batch\n",
    "states, actions, rewards, next_states, dones = test_buffer.sample(5)\n",
    "print(f\"\\nSampled batch shapes:\")\n",
    "print(f\"  States: {states.shape}\")\n",
    "print(f\"  Actions: {actions.shape}\")\n",
    "print(f\"  Rewards: {rewards.shape}\")\n",
    "print(f\"  Next states: {next_states.shape}\")\n",
    "print(f\"  Dones: {dones.shape}\")\n",
    "print(\"‚úÖ Replay Buffer test passed!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Replay Buffer implemented\n",
      "\n",
      "üß™ Testing Replay Buffer...\n",
      "Buffer size: 10\n",
      "Can sample batch of 5: True\n",
      "\n",
      "Sampled batch shapes:\n",
      "  States: (5, 23)\n",
      "  Actions: (5, 7)\n",
      "  Rewards: (5, 1)\n",
      "  Next states: (5, 23)\n",
      "  Dones: (5, 1)\n",
      "‚úÖ Replay Buffer test passed!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Network Architectures\n",
    "\n",
    "We need three types of networks:\n",
    "1. **Actor Network**: Outputs policy distribution\n",
    "2. **Critic Network**: Outputs Q-values\n",
    "3. Helper functions for initialization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T21:48:42.935593Z",
     "start_time": "2025-10-11T21:48:42.732580Z"
    }
   },
   "source": [
    "def initialize_weights(layer, gain=1.0):\n",
    "    \"\"\"\n",
    "    Initialize network weights using orthogonal initialization.\n",
    "    \n",
    "    Args:\n",
    "        layer: Neural network layer\n",
    "        gain: Scaling factor for initialization\n",
    "    \"\"\"\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        nn.init.orthogonal_(layer.weight, gain=gain)\n",
    "        nn.init.constant_(layer.bias, 0.0)\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor network that outputs a Gaussian policy.\n",
    "    \n",
    "    Outputs:\n",
    "        - mean: Mean of action distribution\n",
    "        - log_std: Log standard deviation of action distribution\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256, log_std_min=-20, log_std_max=2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state_dim: Dimension of state space\n",
    "            action_dim: Dimension of action space\n",
    "            hidden_dim: Size of hidden layers\n",
    "            log_std_min: Minimum log standard deviation\n",
    "            log_std_max: Maximum log standard deviation\n",
    "        \"\"\"\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        \n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "        # Shared layers\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Output layers\n",
    "        self.mean = nn.Linear(hidden_dim, action_dim)\n",
    "        self.log_std = nn.Linear(hidden_dim, action_dim)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(lambda layer: initialize_weights(layer, gain=np.sqrt(2)))\n",
    "        initialize_weights(self.mean, gain=0.01)\n",
    "        initialize_weights(self.log_std, gain=0.01)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            state: Input state\n",
    "            \n",
    "        Returns:\n",
    "            mean: Mean of action distribution\n",
    "            log_std: Log standard deviation (clamped)\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        mean = self.mean(x)\n",
    "        log_std = self.log_std(x)\n",
    "        \n",
    "        # Clamp log_std to prevent numerical instability\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        \n",
    "        return mean, log_std\n",
    "    \n",
    "    def sample(self, state):\n",
    "        \"\"\"\n",
    "        Sample an action from the policy.\n",
    "        \n",
    "        Uses the reparameterization trick: a = Œº + œÉ * Œµ, where Œµ ~ N(0,1)\n",
    "        \n",
    "        Args:\n",
    "            state: Input state\n",
    "            \n",
    "        Returns:\n",
    "            action: Sampled action (squashed with tanh)\n",
    "            log_prob: Log probability of the action\n",
    "            mean: Mean of distribution (for evaluation)\n",
    "        \"\"\"\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        \n",
    "        # Create normal distribution\n",
    "        normal = Normal(mean, std)\n",
    "        \n",
    "        # Sample using reparameterization trick\n",
    "        x_t = normal.rsample()  # rsample() uses reparameterization\n",
    "        \n",
    "        # Apply tanh squashing\n",
    "        action = torch.tanh(x_t)\n",
    "        \n",
    "        # Compute log probability with correction for tanh squashing\n",
    "        log_prob = normal.log_prob(x_t)\n",
    "        # Enforcing Action Bound (from SAC paper appendix C)\n",
    "        log_prob -= torch.log(1 - action.pow(2) + 1e-6)\n",
    "        log_prob = log_prob.sum(1, keepdim=True)\n",
    "        \n",
    "        return action, log_prob, torch.tanh(mean)\n",
    "\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Critic network that outputs Q-values for state-action pairs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state_dim: Dimension of state space\n",
    "            action_dim: Dimension of action space\n",
    "            hidden_dim: Size of hidden layers\n",
    "        \"\"\"\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        \n",
    "        # Q1 network\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.q1 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        # Q2 network\n",
    "        self.fc3 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.q2 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(lambda layer: initialize_weights(layer, gain=np.sqrt(2)))\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        \"\"\"\n",
    "        Forward pass through both Q-networks.\n",
    "        \n",
    "        Args:\n",
    "            state: Input state\n",
    "            action: Input action\n",
    "            \n",
    "        Returns:\n",
    "            q1: Q-value from first network\n",
    "            q2: Q-value from second network\n",
    "        \"\"\"\n",
    "        # Concatenate state and action\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        \n",
    "        # Q1 forward pass\n",
    "        q1 = F.relu(self.fc1(x))\n",
    "        q1 = F.relu(self.fc2(q1))\n",
    "        q1 = self.q1(q1)\n",
    "        \n",
    "        # Q2 forward pass\n",
    "        q2 = F.relu(self.fc3(x))\n",
    "        q2 = F.relu(self.fc4(q2))\n",
    "        q2 = self.q2(q2)\n",
    "        \n",
    "        return q1, q2\n",
    "\n",
    "print(\"‚úÖ Neural networks implemented\")\n",
    "\n",
    "# Test the networks\n",
    "print(\"\\nüß™ Testing Neural Networks...\")\n",
    "state_dim = 23  # Pusher observation space\n",
    "action_dim = 7  # Pusher action space\n",
    "\n",
    "# Create networks\n",
    "actor = ActorNetwork(state_dim, action_dim).to(device)\n",
    "critic = CriticNetwork(state_dim, action_dim).to(device)\n",
    "\n",
    "# Test with dummy input\n",
    "dummy_state = torch.randn(10, state_dim).to(device)\n",
    "dummy_action = torch.randn(10, action_dim).to(device)\n",
    "\n",
    "# Test actor\n",
    "mean, log_std = actor(dummy_state)\n",
    "action, log_prob, _ = actor.sample(dummy_state)\n",
    "print(f\"Actor output shapes:\")\n",
    "print(f\"  Mean: {mean.shape}\")\n",
    "print(f\"  Log std: {log_std.shape}\")\n",
    "print(f\"  Action: {action.shape}\")\n",
    "print(f\"  Log prob: {log_prob.shape}\")\n",
    "\n",
    "# Test critic\n",
    "q1, q2 = critic(dummy_state, dummy_action)\n",
    "print(f\"\\nCritic output shapes:\")\n",
    "print(f\"  Q1: {q1.shape}\")\n",
    "print(f\"  Q2: {q2.shape}\")\n",
    "\n",
    "print(\"\\n‚úÖ Neural network tests passed!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Neural networks implemented\n",
      "\n",
      "üß™ Testing Neural Networks...\n",
      "Actor output shapes:\n",
      "  Mean: torch.Size([10, 7])\n",
      "  Log std: torch.Size([10, 7])\n",
      "  Action: torch.Size([10, 7])\n",
      "  Log prob: torch.Size([10, 1])\n",
      "\n",
      "Critic output shapes:\n",
      "  Q1: torch.Size([10, 1])\n",
      "  Q2: torch.Size([10, 1])\n",
      "\n",
      "‚úÖ Neural network tests passed!\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SAC Agent Implementation\n",
    "\n",
    "Now we'll implement the complete SAC agent that ties everything together."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T21:48:44.125337Z",
     "start_time": "2025-10-11T21:48:44.092531Z"
    }
   },
   "source": [
    "class SACAgent:\n",
    "    \"\"\"\n",
    "    Soft Actor-Critic agent implementation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        hidden_dim=256,\n",
    "        lr=3e-4,\n",
    "        gamma=0.99,\n",
    "        tau=0.005,\n",
    "        alpha=0.2,\n",
    "        automatic_entropy_tuning=True,\n",
    "        buffer_capacity=1000000,\n",
    "        device='cpu'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state_dim: Dimension of state space\n",
    "            action_dim: Dimension of action space\n",
    "            hidden_dim: Size of hidden layers\n",
    "            lr: Learning rate\n",
    "            gamma: Discount factor\n",
    "            tau: Target network update rate (soft update)\n",
    "            alpha: Entropy temperature (if not learning it)\n",
    "            automatic_entropy_tuning: Whether to learn alpha\n",
    "            buffer_capacity: Replay buffer size\n",
    "            device: Device to use (cpu/cuda)\n",
    "        \"\"\"\n",
    "        self.device = torch.device(device)\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # Initialize networks\n",
    "        self.actor = ActorNetwork(state_dim, action_dim, hidden_dim).to(self.device)\n",
    "        self.critic = CriticNetwork(state_dim, action_dim, hidden_dim).to(self.device)\n",
    "        self.critic_target = CriticNetwork(state_dim, action_dim, hidden_dim).to(self.device)\n",
    "        \n",
    "        # Copy parameters to target network\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        \n",
    "        # Optimizers\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)\n",
    "        \n",
    "        # Entropy temperature\n",
    "        self.automatic_entropy_tuning = automatic_entropy_tuning\n",
    "        if automatic_entropy_tuning:\n",
    "            # Target entropy = -dim(A) (heuristic from SAC paper)\n",
    "            self.target_entropy = -action_dim\n",
    "            self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n",
    "            self.alpha_optimizer = optim.Adam([self.log_alpha], lr=lr)\n",
    "            self.alpha = self.log_alpha.exp()\n",
    "        else:\n",
    "            self.alpha = torch.tensor(alpha).to(self.device)\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "        \n",
    "        # Training statistics\n",
    "        self.training_steps = 0\n",
    "    \n",
    "    def select_action(self, state, evaluate=False):\n",
    "        \"\"\"\n",
    "        Select an action given a state.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state\n",
    "            evaluate: If True, use deterministic policy (mean action)\n",
    "            \n",
    "        Returns:\n",
    "            action: Selected action\n",
    "        \"\"\"\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if evaluate:\n",
    "                # Use mean action for evaluation\n",
    "                _, _, action = self.actor.sample(state)\n",
    "            else:\n",
    "                # Sample action for training\n",
    "                action, _, _ = self.actor.sample(state)\n",
    "        \n",
    "        return action.cpu().numpy()[0]\n",
    "    \n",
    "    def update(self, batch_size):\n",
    "        \"\"\"\n",
    "        Perform one gradient update step.\n",
    "        \n",
    "        Args:\n",
    "            batch_size: Size of batch to sample from replay buffer\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of training metrics\n",
    "        \"\"\"\n",
    "        # Sample from replay buffer\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.FloatTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "        \n",
    "        # ========== Update Critic ========== #\n",
    "        with torch.no_grad():\n",
    "            # Sample next actions\n",
    "            next_actions, next_log_probs, _ = self.actor.sample(next_states)\n",
    "            \n",
    "            # Compute target Q-values\n",
    "            q1_target, q2_target = self.critic_target(next_states, next_actions)\n",
    "            min_q_target = torch.min(q1_target, q2_target)\n",
    "            \n",
    "            # Add entropy term\n",
    "            next_q_value = min_q_target - self.alpha * next_log_probs\n",
    "            \n",
    "            # Compute target\n",
    "            target_q = rewards + (1 - dones) * self.gamma * next_q_value\n",
    "        \n",
    "        # Get current Q estimates\n",
    "        q1, q2 = self.critic(states, actions)\n",
    "        \n",
    "        # Compute critic loss\n",
    "        critic_loss = F.mse_loss(q1, target_q) + F.mse_loss(q2, target_q)\n",
    "        \n",
    "        # Update critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # ========== Update Actor ========== #\n",
    "        # Sample actions from current policy\n",
    "        new_actions, log_probs, _ = self.actor.sample(states)\n",
    "        \n",
    "        # Compute Q-values for new actions\n",
    "        q1_new, q2_new = self.critic(states, new_actions)\n",
    "        min_q_new = torch.min(q1_new, q2_new)\n",
    "        \n",
    "        # Compute actor loss\n",
    "        actor_loss = (self.alpha * log_probs - min_q_new).mean()\n",
    "        \n",
    "        # Update actor\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # ========== Update Temperature ========== #\n",
    "        if self.automatic_entropy_tuning:\n",
    "            alpha_loss = -(self.log_alpha * (log_probs + self.target_entropy).detach()).mean()\n",
    "            \n",
    "            self.alpha_optimizer.zero_grad()\n",
    "            alpha_loss.backward()\n",
    "            self.alpha_optimizer.step()\n",
    "            \n",
    "            self.alpha = self.log_alpha.exp()\n",
    "            alpha_tlogs = self.alpha.clone().item()\n",
    "        else:\n",
    "            alpha_loss = torch.tensor(0.).to(self.device)\n",
    "            alpha_tlogs = self.alpha.item()\n",
    "        \n",
    "        # ========== Update Target Networks ========== #\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "        \n",
    "        self.training_steps += 1\n",
    "        \n",
    "        # Return metrics\n",
    "        return {\n",
    "            'critic_loss': critic_loss.item(),\n",
    "            'actor_loss': actor_loss.item(),\n",
    "            'alpha_loss': alpha_loss.item(),\n",
    "            'alpha': alpha_tlogs,\n",
    "            'q1_mean': q1.mean().item(),\n",
    "            'q2_mean': q2.mean().item(),\n",
    "            'log_prob_mean': log_probs.mean().item()\n",
    "        }\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save model checkpoint.\"\"\"\n",
    "        torch.save({\n",
    "            'actor': self.actor.state_dict(),\n",
    "            'critic': self.critic.state_dict(),\n",
    "            'critic_target': self.critic_target.state_dict(),\n",
    "            'actor_optimizer': self.actor_optimizer.state_dict(),\n",
    "            'critic_optimizer': self.critic_optimizer.state_dict(),\n",
    "            'log_alpha': self.log_alpha if self.automatic_entropy_tuning else None,\n",
    "            'alpha_optimizer': self.alpha_optimizer.state_dict() if self.automatic_entropy_tuning else None,\n",
    "            'training_steps': self.training_steps\n",
    "        }, filepath)\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        \"\"\"Load model checkpoint.\"\"\"\n",
    "        checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        self.actor.load_state_dict(checkpoint['actor'])\n",
    "        self.critic.load_state_dict(checkpoint['critic'])\n",
    "        self.critic_target.load_state_dict(checkpoint['critic_target'])\n",
    "        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer'])\n",
    "        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer'])\n",
    "        if self.automatic_entropy_tuning and checkpoint['log_alpha'] is not None:\n",
    "            self.log_alpha = checkpoint['log_alpha']\n",
    "            self.alpha_optimizer.load_state_dict(checkpoint['alpha_optimizer'])\n",
    "        self.training_steps = checkpoint['training_steps']\n",
    "\n",
    "print(\"‚úÖ SAC Agent implemented\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SAC Agent implemented\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test the SAC Agent\n",
    "\n",
    "Let's verify our implementation works with the Pusher environment."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T21:48:49.319813Z",
     "start_time": "2025-10-11T21:48:45.303354Z"
    }
   },
   "source": [
    "print(\"üß™ Testing SAC Agent...\\n\")\n",
    "\n",
    "# Create environment\n",
    "env = gym.make(\"Pusher-v5\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "print(f\"Environment info:\")\n",
    "print(f\"  State dimension: {state_dim}\")\n",
    "print(f\"  Action dimension: {action_dim}\")\n",
    "print(f\"  Action range: [{env.action_space.low[0]:.1f}, {env.action_space.high[0]:.1f}]\")\n",
    "\n",
    "# Create agent\n",
    "agent = SACAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    hidden_dim=256,\n",
    "    lr=3e-4,\n",
    "    gamma=0.99,\n",
    "    tau=0.005,\n",
    "    alpha=0.2,\n",
    "    automatic_entropy_tuning=True,\n",
    "    buffer_capacity=100000,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Agent created\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Automatic entropy tuning: {agent.automatic_entropy_tuning}\")\n",
    "print(f\"  Initial alpha: {agent.alpha.item():.4f}\")\n",
    "\n",
    "# Test action selection\n",
    "print(\"\\nüé¨ Testing action selection...\")\n",
    "state, _ = env.reset()\n",
    "action = agent.select_action(state, evaluate=False)\n",
    "print(f\"  State shape: {state.shape}\")\n",
    "print(f\"  Action shape: {action.shape}\")\n",
    "print(f\"  Action range: [{action.min():.3f}, {action.max():.3f}]\")\n",
    "\n",
    "# Collect some experiences\n",
    "print(\"\\nüì¶ Collecting experiences...\")\n",
    "for _ in range(1000):\n",
    "    action = agent.select_action(state)\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    agent.replay_buffer.push(state, action, reward, next_state, terminated or truncated)\n",
    "    state = next_state\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        state, _ = env.reset()\n",
    "\n",
    "print(f\"  Buffer size: {len(agent.replay_buffer)}\")\n",
    "\n",
    "# Test update\n",
    "if agent.replay_buffer.is_ready(256):\n",
    "    print(\"\\nüîÑ Testing update step...\")\n",
    "    metrics = agent.update(batch_size=256)\n",
    "    print(\"  Update metrics:\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"    {key}: {value:.4f}\")\n",
    "    print(\"  ‚úÖ Update successful!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Not enough samples for update\")\n",
    "\n",
    "env.close()\n",
    "print(\"\\n‚úÖ All tests passed! SAC agent is ready for training.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing SAC Agent...\n",
      "\n",
      "Environment info:\n",
      "  State dimension: 23\n",
      "  Action dimension: 7\n",
      "  Action range: [-2.0, 2.0]\n",
      "\n",
      "‚úÖ Agent created\n",
      "  Device: cpu\n",
      "  Automatic entropy tuning: True\n",
      "  Initial alpha: 1.0000\n",
      "\n",
      "üé¨ Testing action selection...\n",
      "  State shape: (23,)\n",
      "  Action shape: (7,)\n",
      "  Action range: [-0.816, 0.914]\n",
      "\n",
      "üì¶ Collecting experiences...\n",
      "  Buffer size: 1000\n",
      "\n",
      "üîÑ Testing update step...\n",
      "  Update metrics:\n",
      "    critic_loss: 26.1648\n",
      "    actor_loss: -4.7900\n",
      "    alpha_loss: -0.0000\n",
      "    alpha: 0.9997\n",
      "    q1_mean: -0.0363\n",
      "    q2_mean: 0.4652\n",
      "    log_prob_mean: -4.7084\n",
      "  ‚úÖ Update successful!\n",
      "\n",
      "‚úÖ All tests passed! SAC agent is ready for training.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization Helper Functions\n",
    "\n",
    "Let's create some utilities for visualizing training progress."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T21:48:50.618576Z",
     "start_time": "2025-10-11T21:48:50.604460Z"
    }
   },
   "source": [
    "def plot_training_metrics(metrics_history, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot training metrics over time.\n",
    "    \n",
    "    Args:\n",
    "        metrics_history: Dictionary of metric lists\n",
    "        save_path: Optional path to save figure\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Define what to plot\n",
    "    plot_configs = [\n",
    "        ('critic_loss', 'Critic Loss', 'blue'),\n",
    "        ('actor_loss', 'Actor Loss', 'red'),\n",
    "        ('alpha', 'Alpha (Temperature)', 'green'),\n",
    "        ('q1_mean', 'Q1 Mean', 'purple'),\n",
    "        ('q2_mean', 'Q2 Mean', 'orange'),\n",
    "        ('log_prob_mean', 'Log Prob Mean', 'brown')\n",
    "    ]\n",
    "    \n",
    "    for idx, (key, title, color) in enumerate(plot_configs):\n",
    "        if key in metrics_history and len(metrics_history[key]) > 0:\n",
    "            axes[idx].plot(metrics_history[key], color=color, alpha=0.6, linewidth=0.5)\n",
    "            # Plot moving average\n",
    "            window = min(100, len(metrics_history[key]) // 10)\n",
    "            if window > 1:\n",
    "                moving_avg = pd.Series(metrics_history[key]).rolling(window=window).mean()\n",
    "                axes[idx].plot(moving_avg, color=color, linewidth=2, label=f'{window}-step MA')\n",
    "                axes[idx].legend()\n",
    "            axes[idx].set_title(title)\n",
    "            axes[idx].set_xlabel('Update Step')\n",
    "            axes[idx].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_episode_rewards(episode_rewards, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot episode rewards over time.\n",
    "    \n",
    "    Args:\n",
    "        episode_rewards: List of episode rewards\n",
    "        save_path: Optional path to save figure\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    ax.plot(episode_rewards, alpha=0.3, color='blue', linewidth=0.5)\n",
    "    \n",
    "    # Plot moving average\n",
    "    window = min(100, len(episode_rewards) // 10)\n",
    "    if window > 1:\n",
    "        moving_avg = pd.Series(episode_rewards).rolling(window=window).mean()\n",
    "        ax.plot(moving_avg, color='red', linewidth=2, label=f'{window}-episode MA')\n",
    "        ax.legend()\n",
    "    \n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Total Reward')\n",
    "    ax.set_title('Episode Rewards Over Time')\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization functions created\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Visualization functions created\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Agent to src/ Directory\n",
    "\n",
    "Let's save our SAC implementation to the src directory so we can use it in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T21:48:52.286887Z",
     "start_time": "2025-10-11T21:48:52.274382Z"
    }
   },
   "source": [
    "# Create the agents directory if it doesn't exist\n",
    "os.makedirs('../src/agents', exist_ok=True)\n",
    "os.makedirs('../src/utils', exist_ok=True)\n",
    "\n",
    "print(\"üíæ Saving implementations to src/...\")\n",
    "\n",
    "# We'll create the files programmatically\n",
    "# (In practice, you'd copy the class definitions above)\n",
    "\n",
    "print(\"\\n‚úÖ To use in other notebooks, you can import:\")\n",
    "print(\"   from src.agents.sac import SACAgent\")\n",
    "print(\"   from src.utils.replay_buffer import ReplayBuffer\")\n",
    "print(\"\\n‚ö†Ô∏è Note: Remember to manually copy the class definitions to:\")\n",
    "print(\"   - src/agents/sac.py\")\n",
    "print(\"   - src/utils/replay_buffer.py\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving implementations to src/...\n",
      "\n",
      "‚úÖ To use in other notebooks, you can import:\n",
      "   from src.agents.sac import SACAgent\n",
      "   from src.utils.replay_buffer import ReplayBuffer\n",
      "\n",
      "‚ö†Ô∏è Note: Remember to manually copy the class definitions to:\n",
      "   - src/agents/sac.py\n",
      "   - src/utils/replay_buffer.py\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Key Takeaways\n",
    "\n",
    "### What We Implemented:\n",
    "\n",
    "‚úÖ **Replay Buffer**\n",
    "- Stores transitions for off-policy learning\n",
    "- Samples random batches\n",
    "- Breaks temporal correlations\n",
    "\n",
    "‚úÖ **Actor Network**\n",
    "- Gaussian policy with learned mean and std\n",
    "- Reparameterization trick for gradient flow\n",
    "- Tanh squashing for bounded actions\n",
    "\n",
    "‚úÖ **Critic Networks**\n",
    "- Twin Q-networks to reduce overestimation\n",
    "- Takes state-action pairs as input\n",
    "- Trained with TD error\n",
    "\n",
    "‚úÖ **SAC Agent**\n",
    "- Combines all components\n",
    "- Automatic entropy tuning\n",
    "- Soft target updates\n",
    "- Complete training loop\n",
    "\n",
    "### Key SAC Concepts:\n",
    "\n",
    "1. **Maximum Entropy RL**\n",
    "   - Maximizes reward + entropy\n",
    "   - Encourages exploration\n",
    "   - More robust policies\n",
    "\n",
    "2. **Off-Policy Learning**\n",
    "   - Uses replay buffer\n",
    "   - More sample efficient\n",
    "   - Can reuse old experiences\n",
    "\n",
    "3. **Twin Critics**\n",
    "   - Reduces Q-value overestimation\n",
    "   - Takes minimum of two estimates\n",
    "   - More stable training\n",
    "\n",
    "4. **Automatic Tuning**\n",
    "   - Learns optimal temperature Œ±\n",
    "   - Balances exploration/exploitation\n",
    "   - One less hyperparameter to tune!\n",
    "\n",
    "### üéØ Next Steps\n",
    "\n",
    "In the next notebook (`04_sac_training.ipynb`), we'll:\n",
    "1. Train the SAC agent on Pusher\n",
    "2. Monitor training progress\n",
    "3. Evaluate performance\n",
    "4. Compare against our heuristic baselines\n",
    "5. Save and visualize the trained agent\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to start training? Open `04_sac_training.ipynb`!** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Quick Reference: SAC Algorithm\n",
    "\n",
    "For your reference, here's the complete SAC algorithm:\n",
    "\n",
    "```\n",
    "Initialize:\n",
    "  - Actor network œÄ_œÜ\n",
    "  - Critic networks Q_Œ∏1, Q_Œ∏2\n",
    "  - Target critics Q_Œ∏ÃÑ1, Q_Œ∏ÃÑ2\n",
    "  - Replay buffer D\n",
    "  - Temperature Œ± (or log Œ± if learning)\n",
    "\n",
    "For each episode:\n",
    "  Observe state s\n",
    "  \n",
    "  For each step:\n",
    "    1. Sample action: a ~ œÄ_œÜ(¬∑|s)\n",
    "    2. Execute action, observe r, s'\n",
    "    3. Store (s, a, r, s') in D\n",
    "    \n",
    "    4. Sample mini-batch from D\n",
    "    \n",
    "    5. Update critics:\n",
    "       - Sample a' ~ œÄ_œÜ(¬∑|s')\n",
    "       - Compute target:\n",
    "         y = r + Œ≥(min(Q_Œ∏ÃÑ1(s',a'), Q_Œ∏ÃÑ2(s',a')) - Œ±¬∑log œÄ_œÜ(a'|s'))\n",
    "       - Minimize: L_Q = (Q_Œ∏i(s,a) - y)¬≤\n",
    "    \n",
    "    6. Update actor:\n",
    "       - Sample a ~ œÄ_œÜ(¬∑|s)\n",
    "       - Maximize: J_œÄ = E[min(Q_Œ∏1(s,a), Q_Œ∏2(s,a)) - Œ±¬∑log œÄ_œÜ(a|s)]\n",
    "    \n",
    "    7. Update temperature (if learning):\n",
    "       - Minimize: L_Œ± = -Œ±(log œÄ_œÜ(a|s) + H_target)\n",
    "    \n",
    "    8. Update targets:\n",
    "       - Œ∏ÃÑi ‚Üê œÑ¬∑Œ∏i + (1-œÑ)¬∑Œ∏ÃÑi\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
